import sys
import os
import argparse
import json
from tqdm import tqdm
import pickle
import random
import re
import multiprocessing
import cProfile, pstats
import shutil

import eval_utils


# Custom objects know their class.
# Function objects seem to know way too much, including modules.
# Exclude modules as well.

essential_keys = ['diac', 'lex', 'pos', 'asp', 'mod', 'vox', 'per', 'num', 'gen',
                  'cas', 'stt', 'prc0', 'prc1', 'prc2', 'prc3', 'enc0']
feats_oblig = ['asp', 'mod', 'vox', 'per', 'num', 'gen', 'cas', 'stt']
essential_keys_no_lex_pos = [k for k in essential_keys if k not in ['diac', 'lex', 'pos']]

parser = argparse.ArgumentParser()
parser.add_argument("-output_dir", default='eval_files',
                    type=str, help="Path of the directory to output evaluation results.")
parser.add_argument("-config_file", default='configs/config.json',
                    type=str, help="Config file specifying which sheets to use.")
parser.add_argument("-db_dir", default='databases',
                        type=str, help="Path of the directory to load the DB from.")
parser.add_argument("-msa_config_name", default='all_aspects_msa',
                    type=str, help="Config name which specifies the path of the MSA Camel DB.")
parser.add_argument("-msa_baseline_db", default='eval_files/calima-msa-s31_0.4.2.utf8.db',
                    type=str, help="Path of the MSA baseline DB file we will be comparing against.")
parser.add_argument("-multiprocessing", default=False, action='store_true',
                    help="Whether or not to use multiprocessing.")
parser.add_argument("-report_dir", default='eval_files/report_default',
                    type=str, help="Paths of the directory containing partial reports generated by the full generative evaluation.")
parser.add_argument("-test_mode", default=False, action='store_true',
                    help="Only test mode.")
parser.add_argument("-profiling", default=False, action='store_true',
                    help="Run profiling.")
parser.add_argument("-chunk", default=100,
                    type=int, help="Length of chunks that get processed in parallel and then aggregated.")
parser.add_argument("-n_cpu", default=8,
                    type=int, help="Number of cores to use.")
parser.add_argument("-n", default=100,
                    type=int, help="Number of inputs to the two compared systems.")
parser.add_argument("-camel_tools", default='local', choices=['local', 'official'],
                    type=str, help="Path of the directory containing the camel_tools modules.")
args = parser.parse_args()

random.seed(42)

with open(args.config_file) as f:
    config = json.load(f)
    config_local = config['local']
    config_msa = config_local[args.msa_config_name]

if args.camel_tools == 'local':
    camel_tools_dir = config['global']['camel_tools']
    sys.path.insert(0, camel_tools_dir)

from camel_tools.morphology.database import MorphologyDB
from camel_tools.morphology.generator import Generator
from camel_tools.utils.charmap import CharMapper

bw2ar = CharMapper.builtin_mapper('bw2ar')
ar2bw = CharMapper.builtin_mapper('ar2bw')

sukun_ar, fatHa_ar = bw2ar('o'), bw2ar('a')
sukun_regex = re.compile(sukun_ar)
aA_regex = re.compile(f'(?<!^[وف]){fatHa_ar}ا')


def get_all_lemmas_from_db(db):
    lemma_pos = set()
    for match, analyses in db.stem_hash.items():
        for cat, analysis in analyses:
            lemma_pos.add((analysis['lex'], analysis['pos']))
    return lemma_pos


def _preprocess_lex_features(analysis):
    for k in ['lex', 'diac']:
        analysis[k] = sukun_regex.sub('', analysis[k])
        analysis[k] = aA_regex.sub('A', analysis[k])


def generate_all_possible_words_from_lemma(id_lemma_pos, oblig_spec=False):
    if not args.multiprocessing:
        global eval_with_clitics, failed
    else:
        eval_with_clitics, failed = {}, {}
    
    lemma_id, (lemma_ar, pos) = id_lemma_pos
    generations_baseline, generations_camel = [], []
    
    if oblig_spec:
        for feats_oblig in tqdm(pos2obligfeats[pos]):
            for feats_clitic in pos2cliticfeats[pos]:
                feats_all = {**feats_oblig, **feats_clitic}
                try:
                    generations_baseline += generator_baseline.generate(lemma_ar, feats_all)
                except:
                    failed.setdefault('baseline', []).append((lemma_ar, feats_all))
                if feats_all.get('prc0') not in ['mA_neg', 'lA_neg']:
                    try:
                        generations_camel += generator_camel.generate(lemma_ar, feats_all)
                    except:
                        failed.setdefault('camel', []).append((lemma_ar, feats_all))
    else:
        for feats_clitic in pos2cliticfeats[pos]:
            try:
                generations_baseline += generator_baseline.generate(lemma_ar, feats_clitic)
            except:
                failed.setdefault('baseline', []).append((lemma_ar, feats_clitic))
            if feats_clitic.get('prc0') not in ['mA_neg', 'lA_neg']:
                try:
                    generations_ = generator_camel.generate(lemma_ar, feats_clitic)
                    generations_camel += generations_
                except:
                    failed.setdefault('camel', []).append((lemma_ar, feats_clitic))

    
    generations_baseline_ = {}
    for g in generations_baseline:
        _preprocess_lex_features(g)
        key = tuple([g.get(k, 'na') for k in essential_keys_no_lex_pos])
        generations_baseline_.setdefault(key, []).append((g['lex'], g['diac'], g['pos']))
    
    generations_camel_ = {}
    for g in generations_camel:
        _preprocess_lex_features(g)
        key = tuple([g.get(k, 'na') for k in essential_keys_no_lex_pos])
        generations_camel_.setdefault(key, []).append((g['lex'], g['diac'], g['pos']))

    generations_camel_set, generations_baseline_set = set(generations_camel_), set(generations_baseline_)
    camel_minus_baseline = generations_camel_set - generations_baseline_set
    baseline_minus_camel = generations_baseline_set - generations_camel_set
    intersection = generations_camel_set & generations_baseline_set
    
    for k in camel_minus_baseline:
        eval_with_clitics.setdefault('camel', {}).setdefault('+'.join(k), []).append(lemma_id)
    for k in baseline_minus_camel:
        eval_with_clitics.setdefault('baseline', {}).setdefault('+'.join(k), []).append(lemma_id)
    for k in intersection:
        info = eval_with_clitics.setdefault('both', {}).setdefault(
            '+'.join(k), {'matching': 0, 'total': 0, 'intersect_baseline': [0, []], 'intersect_camel': [0, []], 'no_intersect': [0, []]})
        lemma_diac_pos_baseline_set = set(generations_baseline_[k])
        lemma_diac_pos_camel_set = set(generations_camel_[k])
        info['matching'] += len(lemma_diac_pos_camel_set & lemma_diac_pos_baseline_set)
        info['total'] += len(lemma_diac_pos_baseline_set)
        
        camel_baseline_lex_intersect = lemma_diac_pos_camel_set & lemma_diac_pos_baseline_set
        
        camel_minus_baseline_lex = lemma_diac_pos_camel_set - lemma_diac_pos_baseline_set
        if camel_baseline_lex_intersect:
            if camel_minus_baseline_lex:
                info['intersect_camel'][0] += len(camel_minus_baseline_lex)
                info['intersect_camel'][1].append(lemma_id)
            
            baseline_minus_camel_lex = lemma_diac_pos_baseline_set - lemma_diac_pos_camel_set
            if baseline_minus_camel_lex:
                info['intersect_baseline'][0] += len(baseline_minus_camel_lex)
                info['intersect_baseline'][1].append(lemma_id)
        else:
            info['no_intersect'][0] += len(lemma_diac_pos_baseline_set)
            info['no_intersect'][1].append(lemma_id)

    return eval_with_clitics


path_db_baseline = args.msa_baseline_db
db_baseline = MorphologyDB(path_db_baseline)

db_baseline_gen = MorphologyDB(path_db_baseline, flags='g')
generator_baseline = Generator(db_baseline_gen)

path_db_camel = os.path.join(args.db_dir, config_msa['db'])
db_camel_gen = MorphologyDB(path_db_camel, flags='g')
generator_camel = Generator(db_camel_gen)

DEFINES = {k: v if v is None else [vv for vv in v if vv != 'na']
           for k, v in generator_baseline._db.defines.items()}

pos2cliticfeats = eval_utils.get_pos2clitic_combs(db_baseline)
pos2obligfeats = eval_utils._get_pos2obligfeats(db_baseline)

eval_with_clitics = {}
failed = {}

if args.profiling:
    profiler = cProfile.Profile()
    profiler.enable()

if __name__ == "__main__":
    lemmas_pos = list(get_all_lemmas_from_db(db_baseline))
    lemmas_pos = [(i, lemma_pos) for i, lemma_pos in enumerate(lemmas_pos)
                    if lemma_pos[-1] == 'verb'][:args.n]
    if args.multiprocessing:
        chunk_size = args.chunk
        lemma_chunked = [lemmas_pos[i:i + chunk_size]
                            for i in range(0, len(lemmas_pos), chunk_size)]
        dump_count = 0
        if os.path.isdir(args.report_dir):
            shutil.rmtree(args.report_dir)
        os.makedirs(args.report_dir)
        
        with open(os.path.join(args.report_dir, 'lemmas_pos.pkl'), 'wb') as f:
            pickle.dump(lemmas_pos, f)
        
        for i_chunk, chunk in enumerate(tqdm(lemma_chunked)):
            with multiprocessing.Pool(args.n_cpu) as p:
                results = list(p.imap(generate_all_possible_words_from_lemma, chunk))
            for eval_with_clitics_ in results:
                eval_utils.join_reports(
                    main_report=eval_with_clitics, report_to_add=eval_with_clitics_)
            if i_chunk % int(len(lemma_chunked) / 4) == 0 or i_chunk == len(lemma_chunked) - 1:
                with open(os.path.join(args.report_dir, f'{str(dump_count)}.pkl'), 'wb') as f:
                    pickle.dump(eval_with_clitics, f)
                dump_count += 1
                eval_with_clitics = {}

        with open(os.path.join(args.report_dir, 'failed.pkl'), 'wb') as f:
            pickle.dump(failed, f)
        
    else:
        results = []
        for id_lemma_pos in tqdm(lemmas_pos):
            generate_all_possible_words_from_lemma(id_lemma_pos)

        with open(os.path.join(args.report_dir, 'results_serial.pkl'), 'wb') as f:
            pickle.dump(eval_with_clitics, f)